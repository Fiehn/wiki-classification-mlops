{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'evidently.dashboard'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mevidently\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Report\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mevidently\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetric_preset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataDriftPreset, DataQualityPreset, TargetDriftPreset\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mevidently\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdashboard\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dashboard\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mevidently\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdashboard\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtabs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     16\u001b[0m     DataDriftTab,\n\u001b[1;32m     17\u001b[0m     CatTargetDriftTab,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m     ProbClassificationPerformanceTab,\n\u001b[1;32m     21\u001b[0m )\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Local imports\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'evidently.dashboard'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import logging\n",
    "import wandb\n",
    "import typer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from torch_geometric.datasets import WikiCS\n",
    "from google.cloud import storage\n",
    "from google.cloud import secretmanager\n",
    "from evidently.report import Report\n",
    "from evidently.metric_preset import DataDriftPreset, DataQualityPreset, TargetDriftPreset\n",
    "from evidently.dashboard import Dashboard\n",
    "from evidently.dashboard.tabs import (\n",
    "    DataDriftTab,\n",
    "    CatTargetDriftTab,\n",
    "    RegressionPerformanceTab,\n",
    "    ClassificationPerformanceTab,\n",
    "    ProbClassificationPerformanceTab,\n",
    ")\n",
    "\n",
    "# Local imports\n",
    "from model import NodeLevelGNN\n",
    "\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"dtumlops-448012-37e77e52cd8f.json\"\n",
    "\n",
    "def get_secret(secret_name):\n",
    "    # Create the Secret Manager client\n",
    "    client = secretmanager.SecretManagerServiceClient()\n",
    "    \n",
    "    # Access the secret version\n",
    "    project_id = \"dtumlops-448012\"\t\n",
    "    name = f\"projects/{project_id}/secrets/{secret_name}/versions/latest\"\n",
    "    response = client.access_secret_version(name=name)\n",
    "    \n",
    "    # Decode the secret payload\n",
    "    secret = response.payload.data.decode('UTF-8')\n",
    "    return secret\n",
    "\n",
    "\n",
    "if \"WANDB_API_KEY\" in os.environ or wandb.api.api_key == \"\":\n",
    "# if os.environ.get(\"WANDB_API_KEY\") == \"\" or os.environ.get(\"WANDB_API_KEY\") == None or wandb.api.api_key == \"\":  \n",
    "    # Get the WandB API key from Secret Manager\n",
    "    wandb_api_key = get_secret(\"WANDB_API_KEY\")\n",
    "\n",
    "    # Log in to WandB using the API key\n",
    "    os.environ[\"WANDB_API_KEY\"] = wandb_api_key\n",
    "    \n",
    "app = typer.Typer()\n",
    "\n",
    "\n",
    "def download_from_gcs(bucket_name, source_folder, destination_folder):\n",
    "    \"\"\"Download files from a GCS bucket.\"\"\"\n",
    "\n",
    "    if \"GOOGLE_APPLICATION_CREDENTIALS\" not in os.environ and os.path.exists(\"cloud/dtumlops-448012-37e77e52cd8f.json\"):\n",
    "        os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"cloud/dtumlops-448012-37e77e52cd8f.json\"\n",
    "\n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket(bucket_name)\n",
    "\n",
    "    # Ensure destination folder exists\n",
    "    os.makedirs(destination_folder, exist_ok=True)\n",
    "\n",
    "    blobs = bucket.list_blobs(prefix=source_folder)\n",
    "    # print(\"Items in bucket:\", [blob.name for blob in blobs])\n",
    "    for blob in blobs:\n",
    "        # Skip directories\n",
    "        if blob.name.endswith(\"/\"):\n",
    "            continue\n",
    "\n",
    "        # Construct the file path relative to the destination folder\n",
    "        file_path = os.path.join(destination_folder, os.path.relpath(blob.name, source_folder))\n",
    "\n",
    "        # Ensure the directory for the file exists\n",
    "        os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "\n",
    "        # Download the file to the constructed file path\n",
    "        blob.download_to_filename(file_path)\n",
    "        print(f\"Downloaded {blob.name} to {file_path}\")\n",
    "\n",
    "    return destination_folder\n",
    "\n",
    "\n",
    " # Download original data from GCS\n",
    "data_path = download_from_gcs(\"mlops-proj-group3-bucket\", \"torch_geometric_data\", \"data\")\n",
    "data_module = WikiCS(root=data_path, is_undirected=True)\n",
    "reference_data = data_module[0]\n",
    "# Extract features and convert to DataFrame\n",
    "reference_features = reference_data.x.numpy()  # Convert to NumPy\n",
    "reference_df = pd.DataFrame(reference_features)\n",
    "print(\"Reference Data:\", reference_df.head())\n",
    "\n",
    "\n",
    "# Download new data\n",
    "data_path_current = download_from_gcs(\"mlops-proj-group3-bucket\", \"userinput\", \"data\")\n",
    "\n",
    "all_data = [] # Initialize a list to store all data\n",
    "# Iterate through all JSON files in the folder\n",
    "for file_name in os.listdir(data_path_current): \n",
    "    if file_name.endswith(\".json\"):\n",
    "        current_file = os.path.join(data_path_current, file_name)\n",
    "        \n",
    "        # Load the JSON file\n",
    "        with open(current_file, \"r\") as f:\n",
    "            current_data = json.load(f)\n",
    "    \n",
    "        all_data.append(current_data)\n",
    "\n",
    "# Combine all data into a single DataFrame\n",
    "all_features = []\n",
    "\n",
    "for data in all_data:\n",
    "    features = np.array(data[\"x\"])  # Extract features\n",
    "    all_features.append(features)\n",
    "\n",
    "# Concatenate all feature arrays into a single array\n",
    "combined_features = np.vstack(all_features)\n",
    "\n",
    "# Convert combined features to a DataFrame\n",
    "current_df = pd.DataFrame(combined_features)\n",
    "\n",
    "# Debugging: Print shapes and column names\n",
    "print(\"Reference DataFrame Shape:\", reference_df.shape)\n",
    "print(\"Reference DataFrame Columns:\", reference_df.columns.tolist())\n",
    "\n",
    "print(\"Current DataFrame Shape:\", current_df.shape)\n",
    "print(\"Current DataFrame Columns (before alignment):\", current_df.columns.tolist())\n",
    "\n",
    "\n",
    "# Align schemas and data types\n",
    "current_df = current_df.astype(np.float32)\n",
    "current_df.columns = reference_df.columns\n",
    "\n",
    "# Initialize the report\n",
    "drift_dashboard = Dashboard(tabs=[DataDriftTab()])\n",
    "drift_dashboard.calculate(reference_df, current_df)\n",
    "\n",
    "# report = Report(metrics=[DataDriftPreset(), DataQualityPreset(), TargetDriftPreset()])\n",
    "# report.run(reference_data=reference_df, current_data=current_df) # Run the report\n",
    "# dashboard = Dashboard(tabs=[report])\n",
    "# dashboard.show() \n",
    "\n",
    "#report.as_dict()\n",
    "#report.save(\"data_drift_report\") # Save the report as a JSON file\n",
    "\n",
    "#report.save_html(\"data_drift_report.html\") # Save the report as an HTML file\n",
    "#report.json()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
